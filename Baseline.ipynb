{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecSeats\n",
    "\n",
    "## Baseline : MNL Model\n",
    "\n",
    "Implementation of the Multinomial Logit Model (MNL) for locational choice experiment data, with statsmodels library.\n",
    "\n",
    "**Author of the code:** Anon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librairies import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function from other files import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing.data_loading import load_data_matrix\n",
    "from src.preprocessing.compute_features import Feature_Pipeline\n",
    "from src.utils import Params\n",
    "\n",
    "from src.model.user_specific import *\n",
    "from src.metrics import *\n",
    "from src.room_transformation import *\n",
    "\n",
    "from src.visualisation.plot_example import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File setting\n",
    "\n",
    "Changing the `IND_FILE` value allows to change the studied dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the file to study:\n",
    "IND_FILE = 1\n",
    "\n",
    "params_list = [\"study2.json\", #0\n",
    "               \"study4_CF_FC.json\", #1\n",
    "              ]\n",
    "\n",
    "path_data = \"./data/Blanchard/\"\n",
    "\n",
    "params_file = Params(path_data + \"parameters/\" + params_list[IND_FILE])\n",
    "\n",
    "print(\"INSAMPLE : \", params_file.csv_train)\n",
    "print(\"HOLDOUT : \", params_file.csv_valid)\n",
    "print(\"ROOM SIZE : \", params_file.room_size)\n",
    "print(\"PADDING : \", params_file.padding)\n",
    "print(\"PAIRS OF SEATS : \", params_file.is_couple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_inputs = np.load(path_data+\"numpy/\"+params_file.dataloader_train+\"_inputs.npy\", allow_pickle=True)\n",
    "    train_outputs = np.load(path_data+\"numpy/\"+params_file.dataloader_train+\"_outputs.npy\", allow_pickle=True)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Creating data matrix for train set...\", end=\"\")\n",
    "    train_inputs, train_outputs = load_data_matrix(path = path_data + params_file.csv_train, \n",
    "                                                   room_size = params_file.room_size, \n",
    "                                                   padding = 0,\n",
    "                                                   verbose = False,\n",
    "                                                   to_tensor = 0,\n",
    "                                                   is_wso = params_file.is_wso\n",
    "                                                   )\n",
    "    np.save(path_data+\"numpy/\"+params_file.dataloader_train+\"_inputs.npy\", train_inputs)\n",
    "    np.save(path_data+\"numpy/\"+params_file.dataloader_train+\"_outputs.npy\", train_outputs)\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    valid_inputs = np.load(path_data+\"numpy/\"+params_file.dataloader_valid+\"_inputs.npy\", allow_pickle=True)\n",
    "    valid_outputs = np.load(path_data+\"numpy/\"+params_file.dataloader_valid+\"_outputs.npy\", allow_pickle=True)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Creating data matrix for valid set...\", end=\"\")\n",
    "    valid_inputs, valid_outputs = load_data_matrix(path = path_data + params_file.csv_valid, \n",
    "                                                   room_size = params_file.room_size, \n",
    "                                                   padding = 0,\n",
    "                                                   verbose = False,\n",
    "                                                   to_tensor = 0,\n",
    "                                                   is_wso = params_file.is_wso\n",
    "                                                   )\n",
    "    np.save(path_data+\"numpy/\"+params_file.dataloader_valid+\"_inputs.npy\", valid_inputs)\n",
    "    np.save(path_data+\"numpy/\"+params_file.dataloader_valid+\"_outputs.npy\", valid_outputs)\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file contains pairs of seats, then the transformation to predict the seat on the left is applied on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params_file.is_couple:\n",
    "    for i in range(train_inputs.shape[0]):\n",
    "        for j in range(train_inputs.shape[1]):\n",
    "            train_inputs[i][j] = keep_left_seat(train_inputs[i][j])\n",
    "        for j in range(valid_inputs.shape[1]):\n",
    "            valid_inputs[i][j] = keep_left_seat(valid_inputs[i][j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation \n",
    "\n",
    "For customer, we compute the feature matrix, fit the MNL model, and evaluate it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_parametric = Params(\"src/model/parameters/params_user_specific.json\")\n",
    "pipeline = Feature_Pipeline(params_parametric)\n",
    "\n",
    "acc_1_list = []\n",
    "acc_2_list = []\n",
    "acc_3_list = []\n",
    "l1_loss_list = []\n",
    "\n",
    "for idx_client in range(len(train_inputs)):\n",
    "\n",
    "    train_X, train_Y = pipeline.compute_feature(train_inputs[idx_client], train_outputs[idx_client])\n",
    "    flat_train_X = np.concatenate(train_X, axis=0)\n",
    "    flat_train_X = sm.add_constant(flat_train_X, prepend = False)\n",
    "    flat_train_Y = np.concatenate(train_Y, axis=0)\n",
    "    \n",
    "    sizes = [(len(x), len(x[0])) for x in train_inputs[idx_client]]\n",
    "\n",
    "    mdl = sm.MNLogit(flat_train_Y, flat_train_X)\n",
    "    mdl_fit = mdl.fit(method='lbfgs', maxiter = 200)\n",
    "\n",
    "#     print(mdl_fit.params)\n",
    "#     print(mdl_fit.summary())\n",
    "\n",
    "    valid_X, valid_Y = pipeline.compute_feature(valid_inputs[idx_client], valid_outputs[idx_client])\n",
    "    Y_pred = []\n",
    "    l1 = 0\n",
    "    \n",
    "    valid_X = valid_X.tolist()\n",
    "    \n",
    "    for idx_choice in range(len(valid_X)):\n",
    "        valid_X[idx_choice] = sm.add_constant(valid_X[idx_choice], prepend = False)\n",
    "\n",
    "        prob = mdl_fit.predict(valid_X[idx_choice])[:,1]\n",
    "        prob = [x for x in prob if not np.isnan(x)]\n",
    "        sort_prob = np.flip(np.sort(prob))\n",
    "        Y_pred.append([list(prob).index(p) for p in sort_prob])\n",
    "        \n",
    "        label = np.where(np.asarray(valid_Y[idx_choice]) == 1)[0][0]\n",
    "        label_pos = (int(valid_X[idx_choice][label][0] * sizes[idx_choice][1]), \n",
    "                     int(valid_X[idx_choice][label][1] * sizes[idx_choice][0]))\n",
    "        for p in range(len(prob)):\n",
    "            l1 += prob[p] * (abs(label_pos[0] - int(valid_X[idx_choice][p][0] * sizes[idx_choice][1])) \n",
    "                            + abs(label_pos[1] - int(valid_X[idx_choice][p][1] * sizes[idx_choice][0])))\n",
    "    \n",
    "            \n",
    "    acc_1_list.append(top_n_accuracy(valid_Y, Y_pred, n=1))\n",
    "    acc_2_list.append(top_n_accuracy(valid_Y, Y_pred, n=3))\n",
    "    acc_3_list.append(top_n_accuracy(valid_Y, Y_pred, n=5))\n",
    "    l1_loss_list.append(l1/len(valid_Y))\n",
    "    \n",
    "    print(\"\\r{}/{}\".format(idx_client + 1, len(train_inputs)), end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top-1 Accuracy : \", np.mean(acc_1_list))\n",
    "print(\"Top-3 Accuracy : \", np.mean(acc_2_list))\n",
    "print(\"Top-5 Accuracy : \", np.mean(acc_3_list))\n",
    "print(\"Expected L1 loss : \", np.nanmean(l1_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
