{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecSeats\n",
    "\n",
    "## Individual-level component\n",
    "\n",
    "Main file for running all individual-level choice models (LR, SVC, GBT, RF) on Locational choice experiment data.\n",
    "\n",
    "**Author of the code:** Anon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librairies import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import copy\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function from other files import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing.data_loading import load_data_matrix\n",
    "from src.preprocessing.compute_features import Feature_Pipeline\n",
    "from src.utils import Params\n",
    "\n",
    "from src.model.user_specific import *\n",
    "from src.metrics import *\n",
    "from src.room_transformation import *\n",
    "\n",
    "from src.visualisation.plot_example import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File setting\n",
    "\n",
    "Changing the `IND_FILE` value allows to change the studied dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the file to study:\n",
    "IND_FILE = 0\n",
    "\n",
    "params_list = [\"study2.json\", #0\n",
    "               \"study4_CF_FC.json\", #1\n",
    "              ]\n",
    "\n",
    "path_data = \"./data/Blanchard/\"\n",
    "\n",
    "params_file = Params(path_data + \"parameters/\" + params_list[IND_FILE])\n",
    "\n",
    "print(\"INSAMPLE : \", params_file.csv_train)\n",
    "print(\"HOLDOUT : \", params_file.csv_valid)\n",
    "print(\"ROOM SIZE : \", params_file.room_size)\n",
    "print(\"PADDING : \", params_file.padding)\n",
    "print(\"PAIRS OF SEATS : \", params_file.is_couple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    train_inputs = np.load(path_data+\"numpy/\"+params_file.dataloader_train+\"_inputs.npy\", allow_pickle=True)\n",
    "    train_outputs = np.load(path_data+\"numpy/\"+params_file.dataloader_train+\"_outputs.npy\", allow_pickle=True)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Creating data matrix for train set...\", end=\"\")\n",
    "    train_inputs, train_outputs = load_data_matrix(path = path_data + params_file.csv_train, \n",
    "                                                   room_size = params_file.room_size, \n",
    "                                                   padding = 0,\n",
    "                                                   verbose = False,\n",
    "                                                   to_tensor = 0,\n",
    "                                                   is_wso = params_file.is_wso\n",
    "                                                   )\n",
    "    np.save(path_data+\"numpy/\"+params_file.dataloader_train+\"_inputs.npy\", train_inputs)\n",
    "    np.save(path_data+\"numpy/\"+params_file.dataloader_train+\"_outputs.npy\", train_outputs)\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    valid_inputs = np.load(path_data+\"numpy/\"+params_file.dataloader_valid+\"_inputs.npy\", allow_pickle=True)\n",
    "    valid_outputs = np.load(path_data+\"numpy/\"+params_file.dataloader_valid+\"_outputs.npy\", allow_pickle=True)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Creating data matrix for valid set...\", end=\"\")\n",
    "    valid_inputs, valid_outputs = load_data_matrix(path = path_data + params_file.csv_valid, \n",
    "                                                   room_size = params_file.room_size, \n",
    "                                                   padding = 0,\n",
    "                                                   verbose = False,\n",
    "                                                   to_tensor = 0,\n",
    "                                                   is_wso = params_file.is_wso\n",
    "                                                   )\n",
    "    np.save(path_data+\"numpy/\"+params_file.dataloader_valid+\"_inputs.npy\", valid_inputs)\n",
    "    np.save(path_data+\"numpy/\"+params_file.dataloader_valid+\"_outputs.npy\", valid_outputs)\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file contains pairs of seats, then the transformation to predict the seat on the left is applied on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params_file.is_couple:\n",
    "    for i in range(train_inputs.shape[0]):\n",
    "        for j in range(train_inputs.shape[1]):\n",
    "            train_inputs[i][j] = keep_left_seat(train_inputs[i][j])\n",
    "        for j in range(valid_inputs.shape[1]):\n",
    "            valid_inputs[i][j] = keep_left_seat(valid_inputs[i][j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_parametric = Params(\"src/model/parameters/params_user_specific.json\")\n",
    "\n",
    "pipeline = Feature_Pipeline(params_parametric)\n",
    "\n",
    "model = Recommendation(params_parametric, pipeline)\n",
    "\n",
    "scorer = make_scorer(top_n_accuracy)\n",
    "scorer_3 = make_scorer(top_n_accuracy, n = 3)\n",
    "scorer_5 = make_scorer(top_n_accuracy, n = 5)\n",
    "scorer_10 = make_scorer(top_n_accuracy, n = 10)\n",
    "scorer_20 = make_scorer(top_n_accuracy, n = 20)\n",
    "evals_scorer = [scorer, scorer_3, scorer_5]\n",
    "\n",
    "beta_list, train_acc, valid_acc, cross_acc = model.train_evaluate(train_inputs, train_outputs,\n",
    "                                                                  evals_scorer = evals_scorer,\n",
    "                                                                  test_X = valid_inputs,\n",
    "                                                                  test_Y = valid_outputs,\n",
    "                                                                  verbose = False)\n",
    "np.save(\"save/beta_values.npy\", beta_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Loss computation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sizes = np.asarray([[[len(train_inputs[k][i]), len(train_inputs[k][i][0])] for i in range(len(train_inputs[k]))] \n",
    "                          for k in range(len(train_inputs))])\n",
    "valid_sizes = np.asarray([[[len(valid_inputs[k][i]), len(valid_inputs[k][i][0])] for i in range(len(valid_inputs[k]))] \n",
    "                          for k in range(len(valid_inputs))])\n",
    "l1_loss = []\n",
    "\n",
    "for i in range(len(train_inputs)):\n",
    "    train_X, train_Y = model.pipeline.compute_feature(train_inputs[i], train_outputs[i])\n",
    "    valid_X, valid_Y = model.pipeline.compute_feature(valid_inputs[i], valid_outputs[i])\n",
    "\n",
    "    model.fit(train_X, train_Y)\n",
    "\n",
    "    l1_loss.append(weighted_l1(model, valid_X, valid_Y, valid_sizes[i]))\n",
    "    print(\"\\r{}/{}\".format(i + 1, len(train_inputs)), end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_l1(model, valid_X, valid_Y, valid_sizes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(l1_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
